{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mdehghani86/StorytellingData/blob/main/lab2_mab_combined_json_html.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NH9YT4eRfK0j"
      },
      "source": [
        "<div style=\"background: linear-gradient(90deg, #17a2b8 0%, #0e5a63 60%, #0a3d44 100%); color: white; padding: 18px 25px; margin-bottom: 20px;\">\n",
        "    <div style=\"display: flex; justify-content: space-between; align-items: baseline;\">\n",
        "        <h1 style=\"font-family: 'Helvetica Neue', sans-serif; font-size: 24px; margin: 0; font-weight: 300;\">\n",
        "            Lab 2: Multi-Armed Bandits\n",
        "        </h1>\n",
        "        <span style=\"font-size: 11px; opacity: 0.9;\">© Prof. Dehghani</span>\n",
        "    </div>\n",
        "    <p style=\"font-size: 13px; margin-top: 6px; margin-bottom: 0; opacity: 0.9;\">\n",
        "        IE 7295 Reinforcement Learning | Sutton & Barto Chapter 2 | Intermediate Level | 90 minutes\n",
        "    </p>\n",
        "</div>\n",
        "\n",
        "<div style=\"background: white; padding: 15px 20px; margin-bottom: 12px; border-left: 3px solid #17a2b8;\">\n",
        "    <h3 style=\"color: #17a2b8; font-size: 14px; margin: 0 0 8px 0; text-transform: uppercase; letter-spacing: 0.5px;\">Background</h3>\n",
        "    <p style=\"color: #555; line-height: 1.6; margin: 0; font-size: 13px;\">\n",
        "        The multi-armed bandit problem models decision-making under uncertainty. An agent repeatedly chooses among k actions,\n",
        "        receiving numerical rewards from stationary probability distributions. The challenge is balancing exploration\n",
        "        (trying different actions to find the best) with exploitation (choosing the current best action).\n",
        "        This lab reproduces key results from <a href=\"http://incompleteideas.net/book/the-book-2nd.html\" style=\"color: #17a2b8;\">Sutton & Barto (2018)</a>, Chapter 2.\n",
        "    </p>\n",
        "</div>\n",
        "\n",
        "<table style=\"width: 100%; border-spacing: 12px;\">\n",
        "<tr>\n",
        "<td style=\"background: white; padding: 12px 15px; border-top: 3px solid #17a2b8; vertical-align: top; width: 50%;\">\n",
        "    <h4 style=\"color: #17a2b8; font-size: 13px; margin: 0 0 8px 0; font-weight: 600;\">Learning Objectives</h4>\n",
        "    <ul style=\"color: #555; line-height: 1.4; margin: 0; padding-left: 18px; font-size: 12px;\">\n",
        "        <li>Implement the 10-armed testbed</li>\n",
        "        <li>Compare ε-greedy strategies</li>\n",
        "        <li>Analyze optimistic initial values</li>\n",
        "        <li>Reproduce Figures 2.1, 2.2, and 2.3</li>\n",
        "    </ul>\n",
        "</td>\n",
        "<td style=\"background: white; padding: 12px 15px; border-top: 3px solid #00acc1; vertical-align: top; width: 50%;\">\n",
        "    <h4 style=\"color: #00acc1; font-size: 13px; margin: 0 0 8px 0; font-weight: 600;\">Key Concepts</h4>\n",
        "    <div style=\"color: #555; font-size: 12px; line-height: 1.6;\">\n",
        "        <div style=\"padding: 2px 0;\"><code style=\"background: #e0f7fa; padding: 1px 5px; color: #006064;\">q*(a)</code> = true action value</div>\n",
        "        <div style=\"padding: 2px 0;\"><code style=\"background: #e0f7fa; padding: 1px 5px; color: #006064;\">Qt(a)</code> = estimated value at time t</div>\n",
        "        <div style=\"padding: 2px 0;\"><code style=\"background: #e0f7fa; padding: 1px 5px; color: #006064;\">ε-greedy</code> = exploration strategy</div>\n",
        "        <div style=\"padding: 2px 0;\"><code style=\"background: #e0f7fa; padding: 1px 5px; color: #006064;\">α</code> = step-size parameter</div>\n",
        "    </div>\n",
        "</td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8aHLz4UffK0k"
      },
      "source": [
        "## Configuration and Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBGBggC1fK0l"
      },
      "source": [
        "\"\"\"\n",
        "Cell 1: Environment Setup and Configuration\n",
        "Purpose: Import libraries and set visualization parameters\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import Tuple, List\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configurable color scheme - modify these to change plot colors\n",
        "COLORS = {\n",
        "    'greedy': '#008000',      # Green for ε=0\n",
        "    'epsilon_01': '#FF0000',  # Red for ε=0.01\n",
        "    'epsilon_1': '#0000FF',   # Blue for ε=0.1\n",
        "    'optimistic': '#00BFFF',  # Cyan for optimistic\n",
        "    'realistic': '#808080',   # Gray for realistic\n",
        "    'violin': '#7f7f7f'       # Gray for violin plots\n",
        "}\n",
        "\n",
        "# Standard parameters from Sutton & Barto\n",
        "K = 10          # Number of arms\n",
        "STEPS = 1000    # Time steps per run\n",
        "RUNS = 2000     # Number of independent runs\n",
        "\n",
        "# Configure matplotlib\n",
        "plt.rcParams['figure.dpi'] = 100\n",
        "plt.rcParams['font.size'] = 10\n",
        "plt.rcParams['axes.labelsize'] = 11\n",
        "plt.rcParams['axes.titlesize'] = 12\n",
        "plt.rcParams['legend.fontsize'] = 10\n",
        "\n",
        "def pretty_print(title: str, value: any) -> None:\n",
        "    \"\"\"Display formatted output\"\"\"\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"{title}: {value}\")\n",
        "    print(f\"{'='*50}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6chlLKQfK0l"
      },
      "source": [
        "## Part 1: The 10-Armed Testbed (Figure 2.1)\n",
        "\n",
        "The testbed consists of 10 actions with action values $q_*(a), a = 1, ..., 10$, selected from a normal distribution with mean 0 and variance 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqsAbGBDfK0l"
      },
      "source": [
        "\"\"\"\n",
        "Cell 2: Create and Visualize the 10-Armed Testbed\n",
        "Purpose: Generate Figure 2.1 showing reward distributions\n",
        "\"\"\"\n",
        "\n",
        "# Set seed for reproducible example\n",
        "np.random.seed(0)\n",
        "\n",
        "# Generate true action values q*(a) ~ N(0, 1)\n",
        "q_true = np.random.randn(K)\n",
        "\n",
        "# Create Figure 2.1: Reward distributions\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "# Generate reward distribution samples for visualization\n",
        "n_samples = 10000\n",
        "rewards = np.zeros((n_samples, K))\n",
        "for i in range(K):\n",
        "    rewards[:, i] = q_true[i] + np.random.randn(n_samples)\n",
        "\n",
        "# Create violin plots\n",
        "parts = ax.violinplot(rewards, positions=range(1, K+1), widths=0.7,\n",
        "                      showmeans=False, showextrema=False)\n",
        "\n",
        "# Style the violins\n",
        "for pc in parts['bodies']:\n",
        "    pc.set_facecolor(COLORS['violin'])\n",
        "    pc.set_alpha(0.7)\n",
        "    pc.set_edgecolor('black')\n",
        "    pc.set_linewidth(0.5)\n",
        "\n",
        "# Add horizontal lines for true values\n",
        "for i in range(K):\n",
        "    ax.hlines(q_true[i], i+0.7, i+1.3, colors='black', linewidth=1.5)\n",
        "    ax.text(i+1.4, q_true[i], f'$q_*({i+1})$', fontsize=10, va='center')\n",
        "\n",
        "# Formatting\n",
        "ax.set_xlabel('Action', fontsize=12)\n",
        "ax.set_ylabel('Reward\\ndistribution', fontsize=12)\n",
        "ax.set_xticks(range(1, K+1))\n",
        "ax.set_xlim(0.5, K+0.5)\n",
        "ax.set_ylim(-3, 3)\n",
        "ax.axhline(y=0, color='black', linestyle='--', linewidth=0.5, alpha=0.5)\n",
        "ax.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.title('Figure 2.1: The 10-armed testbed', fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "pretty_print(\"Optimal action\", np.argmax(q_true) + 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9HDImY8fK0l"
      },
      "source": [
        "## Part 2: Core Algorithms\n",
        "\n",
        "We implement the fundamental components:\n",
        "- **ε-greedy action selection**: Choose randomly with probability ε, otherwise choose greedily\n",
        "- **Sample-average update**: $Q_{n+1} = Q_n + \\frac{1}{n}[R_n - Q_n]$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46Yklr0LfK0l"
      },
      "source": [
        "\"\"\"\n",
        "Cell 3: Core Algorithm Implementation\n",
        "Purpose: Define action selection and value update methods\n",
        "\"\"\"\n",
        "\n",
        "def create_bandit() -> np.ndarray:\n",
        "    \"\"\"Create a new 10-armed bandit problem\"\"\"\n",
        "    return np.random.randn(K)\n",
        "\n",
        "def get_reward(action: int, q_true: np.ndarray) -> float:\n",
        "    \"\"\"Get reward for an action: R ~ N(q*(a), 1)\"\"\"\n",
        "    return q_true[action] + np.random.randn()\n",
        "\n",
        "def epsilon_greedy(Q: np.ndarray, epsilon: float) -> int:\n",
        "    \"\"\"\n",
        "    ε-greedy action selection\n",
        "\n",
        "    Args:\n",
        "        Q: Current action value estimates\n",
        "        epsilon: Probability of random exploration\n",
        "\n",
        "    Returns:\n",
        "        Selected action index\n",
        "    \"\"\"\n",
        "    if np.random.random() < epsilon:\n",
        "        # Explore: choose random action\n",
        "        return np.random.randint(K)\n",
        "    else:\n",
        "        # Exploit: choose greedy action (break ties randomly)\n",
        "        max_Q = np.max(Q)\n",
        "        return np.random.choice(np.where(Q == max_Q)[0])\n",
        "\n",
        "def update_estimates(Q: np.ndarray, N: np.ndarray, action: int,\n",
        "                    reward: float, alpha: float = None) -> None:\n",
        "    \"\"\"\n",
        "    Update action value estimates\n",
        "\n",
        "    Args:\n",
        "        Q: Action value estimates (modified in place)\n",
        "        N: Action counts\n",
        "        action: Selected action\n",
        "        reward: Observed reward\n",
        "        alpha: Step size (None for sample average)\n",
        "    \"\"\"\n",
        "    N[action] += 1\n",
        "\n",
        "    if alpha is None:\n",
        "        # Sample average: Q = Q + (1/n)[R - Q]\n",
        "        Q[action] += (reward - Q[action]) / N[action]\n",
        "    else:\n",
        "        # Constant step size: Q = Q + α[R - Q]\n",
        "        Q[action] += alpha * (reward - Q[action])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwK8SLz8fK0m"
      },
      "source": [
        "## Part 3: Comparing ε-greedy Methods (Figure 2.2)\n",
        "\n",
        "We compare three ε-greedy variants: ε = 0 (greedy), ε = 0.01, and ε = 0.1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gpn-v-5-fK0m"
      },
      "source": [
        "\"\"\"\n",
        "Cell 4: Run ε-greedy Experiments\n",
        "Purpose: Generate data for Figure 2.2\n",
        "\"\"\"\n",
        "\n",
        "def run_epsilon_greedy_experiment(epsilon: float, runs: int = 2000,\n",
        "                                 steps: int = 1000) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Run ε-greedy bandit experiment\n",
        "\n",
        "    Returns:\n",
        "        (average_rewards, optimal_action_percentage)\n",
        "    \"\"\"\n",
        "    all_rewards = np.zeros((runs, steps))\n",
        "    all_optimal = np.zeros((runs, steps))\n",
        "\n",
        "    for run in range(runs):\n",
        "        # Initialize new problem\n",
        "        q_true = create_bandit()\n",
        "        optimal_action = np.argmax(q_true)\n",
        "\n",
        "        # Initialize estimates\n",
        "        Q = np.zeros(K)\n",
        "        N = np.zeros(K)\n",
        "\n",
        "        # Run steps\n",
        "        for step in range(steps):\n",
        "            # Select action\n",
        "            action = epsilon_greedy(Q, epsilon)\n",
        "\n",
        "            # Get reward\n",
        "            reward = get_reward(action, q_true)\n",
        "\n",
        "            # Update estimates\n",
        "            update_estimates(Q, N, action, reward)\n",
        "\n",
        "            # Record results\n",
        "            all_rewards[run, step] = reward\n",
        "            all_optimal[run, step] = (action == optimal_action)\n",
        "\n",
        "    # Return averages\n",
        "    return np.mean(all_rewards, axis=0), np.mean(all_optimal, axis=0) * 100\n",
        "\n",
        "# Run experiments for different epsilon values\n",
        "print(\"Running experiments for Figure 2.2...\")\n",
        "results = {}\n",
        "\n",
        "for eps, name in [(0, 'greedy'), (0.01, 'epsilon_01'), (0.1, 'epsilon_1')]:\n",
        "    print(f\"  ε = {eps}...\")\n",
        "    avg_reward, pct_optimal = run_epsilon_greedy_experiment(eps, RUNS, STEPS)\n",
        "    results[eps] = (avg_reward, pct_optimal, name)\n",
        "\n",
        "print(\"Complete!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBS3ryWJfK0m"
      },
      "source": [
        "\"\"\"\n",
        "Cell 5: Plot Figure 2.2\n",
        "Purpose: Reproduce Figure 2.2 from Sutton & Barto\n",
        "\"\"\"\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 8))\n",
        "\n",
        "# Plot average rewards\n",
        "for eps in [0.1, 0.01, 0]:  # Order for correct layering\n",
        "    avg_reward, _, name = results[eps]\n",
        "    ax1.plot(avg_reward, color=COLORS[name], label=f'ε = {eps}', linewidth=1.5)\n",
        "\n",
        "ax1.set_ylabel('Average\\nreward', fontsize=11)\n",
        "ax1.set_xlim(0, 1000)\n",
        "ax1.set_ylim(0, 1.5)\n",
        "ax1.legend(loc='lower right')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot optimal action percentage\n",
        "for eps in [0.1, 0.01, 0]:  # Order for correct layering\n",
        "    _, pct_optimal, name = results[eps]\n",
        "    label = f'ε = {eps} (greedy)' if eps == 0 else f'ε = {eps}'\n",
        "    ax2.plot(pct_optimal, color=COLORS[name], label=label, linewidth=1.5)\n",
        "\n",
        "ax2.set_xlabel('Steps', fontsize=11)\n",
        "ax2.set_ylabel('%\\nOptimal\\naction', fontsize=11)\n",
        "ax2.set_xlim(0, 1000)\n",
        "ax2.set_ylim(0, 100)\n",
        "ax2.legend(loc='lower right')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "fig.suptitle('Figure 2.2: Average performance of ε-greedy action-value methods\\non the 10-armed testbed',\n",
        "             fontsize=12, y=0.995)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDCex_kHfK0m"
      },
      "source": [
        "## Part 4: Optimistic Initial Values (Figure 2.3)\n",
        "\n",
        "Optimistic initialization encourages exploration even with ε = 0. We compare:\n",
        "- Optimistic greedy: Q₁ = 5, ε = 0\n",
        "- Realistic ε-greedy: Q₁ = 0, ε = 0.1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8i0a_DDfK0m"
      },
      "source": [
        "\"\"\"\n",
        "Cell 6: Optimistic Initial Values Experiment\n",
        "Purpose: Generate data for Figure 2.3\n",
        "\"\"\"\n",
        "\n",
        "def run_optimistic_experiment(epsilon: float, initial_Q: float, alpha: float = 0.1,\n",
        "                             runs: int = 2000, steps: int = 1000) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Run experiment with specified initial values\n",
        "    Uses constant step size α = 0.1 as per Figure 2.3\n",
        "\n",
        "    Returns:\n",
        "        Percentage of optimal actions at each step\n",
        "    \"\"\"\n",
        "    all_optimal = np.zeros((runs, steps))\n",
        "\n",
        "    for run in range(runs):\n",
        "        # Initialize new problem\n",
        "        q_true = create_bandit()\n",
        "        optimal_action = np.argmax(q_true)\n",
        "\n",
        "        # Initialize with specified values\n",
        "        Q = np.ones(K) * initial_Q\n",
        "        N = np.zeros(K)\n",
        "\n",
        "        # Run steps\n",
        "        for step in range(steps):\n",
        "            # Select action\n",
        "            action = epsilon_greedy(Q, epsilon)\n",
        "\n",
        "            # Get reward\n",
        "            reward = get_reward(action, q_true)\n",
        "\n",
        "            # Update with constant step size\n",
        "            update_estimates(Q, N, action, reward, alpha=alpha)\n",
        "\n",
        "            # Record if optimal\n",
        "            all_optimal[run, step] = (action == optimal_action)\n",
        "\n",
        "    return np.mean(all_optimal, axis=0) * 100\n",
        "\n",
        "# Run experiments\n",
        "print(\"Running experiments for Figure 2.3...\")\n",
        "print(\"  Optimistic greedy (Q₁=5, ε=0)...\")\n",
        "optimistic_greedy = run_optimistic_experiment(epsilon=0, initial_Q=5, alpha=0.1)\n",
        "\n",
        "print(\"  Realistic ε-greedy (Q₁=0, ε=0.1)...\")\n",
        "realistic_egreedy = run_optimistic_experiment(epsilon=0.1, initial_Q=0, alpha=0.1)\n",
        "\n",
        "print(\"Complete!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TlnKgmVYfK0m"
      },
      "source": [
        "\"\"\"\n",
        "Cell 7: Plot Figure 2.3\n",
        "Purpose: Reproduce Figure 2.3 from Sutton & Barto\n",
        "\"\"\"\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "\n",
        "# Plot optimistic greedy\n",
        "plt.plot(optimistic_greedy, color=COLORS['optimistic'],\n",
        "         label='Optimistic, greedy\\n$Q_1 = 5, ε = 0$', linewidth=1.5)\n",
        "\n",
        "# Plot realistic ε-greedy\n",
        "plt.plot(realistic_egreedy, color=COLORS['realistic'],\n",
        "         label='Realistic, ε-greedy\\n$Q_1 = 0, ε = 0.1$', linewidth=1.5)\n",
        "\n",
        "plt.xlabel('Steps', fontsize=11)\n",
        "plt.ylabel('%\\nOptimal\\naction', fontsize=11)\n",
        "plt.xlim(0, 1000)\n",
        "plt.ylim(0, 100)\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.title('Figure 2.3: The effect of optimistic initial action-value estimates on the 10-armed testbed.\\n' +\n",
        "          'Both methods used a constant step-size parameter, α = 0.1.',\n",
        "          fontsize=11)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxzKPNB3fK0m"
      },
      "source": [
        "## Part 5: Binomial Reward Variant\n",
        "\n",
        "A simplified variant using binary rewards, useful for modeling click-through rates or conversion problems."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ER5OLBGVfK0n"
      },
      "source": [
        "\"\"\"\n",
        "Cell 8: Binomial Bandit Implementation\n",
        "Purpose: Demonstrate binary reward variant\n",
        "\"\"\"\n",
        "\n",
        "def create_binomial_bandit() -> np.ndarray:\n",
        "    \"\"\"Create bandit with success probabilities\"\"\"\n",
        "    return np.random.beta(2, 2, K)  # Beta(2,2) gives values around 0.5\n",
        "\n",
        "def get_binomial_reward(action: int, p_true: np.ndarray) -> int:\n",
        "    \"\"\"Binary reward with probability p(a)\"\"\"\n",
        "    return 1 if np.random.random() < p_true[action] else 0\n",
        "\n",
        "def run_binomial_experiment(epsilon: float, runs: int = 1000,\n",
        "                          steps: int = 1000) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Run binomial bandit experiment\n",
        "    \"\"\"\n",
        "    all_rewards = np.zeros((runs, steps))\n",
        "    all_optimal = np.zeros((runs, steps))\n",
        "\n",
        "    for run in range(runs):\n",
        "        # Initialize problem\n",
        "        p_true = create_binomial_bandit()\n",
        "        optimal_action = np.argmax(p_true)\n",
        "\n",
        "        # Initialize estimates\n",
        "        Q = np.zeros(K)\n",
        "        N = np.zeros(K)\n",
        "\n",
        "        for step in range(steps):\n",
        "            # Select action\n",
        "            action = epsilon_greedy(Q, epsilon)\n",
        "\n",
        "            # Get binary reward\n",
        "            reward = get_binomial_reward(action, p_true)\n",
        "\n",
        "            # Update estimates\n",
        "            update_estimates(Q, N, action, reward)\n",
        "\n",
        "            # Record results\n",
        "            all_rewards[run, step] = reward\n",
        "            all_optimal[run, step] = (action == optimal_action)\n",
        "\n",
        "    return np.mean(all_rewards, axis=0), np.mean(all_optimal, axis=0) * 100\n",
        "\n",
        "# Run binomial experiments\n",
        "print(\"Running binomial bandit experiments...\")\n",
        "binomial_results = {}\n",
        "\n",
        "for eps in [0, 0.01, 0.1]:\n",
        "    print(f\"  ε = {eps}...\")\n",
        "    avg_reward, pct_optimal = run_binomial_experiment(eps, runs=1000)\n",
        "    binomial_results[eps] = (avg_reward, pct_optimal)\n",
        "\n",
        "print(\"Complete!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yve8ULrFfK0n"
      },
      "source": [
        "\"\"\"\n",
        "Cell 9: Visualize Binomial Results\n",
        "Purpose: Compare binomial bandit performance\n",
        "\"\"\"\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "# Plot rewards\n",
        "for eps, (avg_reward, _) in binomial_results.items():\n",
        "    name = 'greedy' if eps == 0 else f'epsilon_{int(eps*100):02d}'\n",
        "    ax1.plot(avg_reward, color=COLORS[name], label=f'ε = {eps}', linewidth=1.5)\n",
        "\n",
        "ax1.set_xlabel('Steps')\n",
        "ax1.set_ylabel('Average Reward')\n",
        "ax1.set_title('Binomial Bandit: Average Reward')\n",
        "ax1.legend(loc='lower right')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot optimal actions\n",
        "for eps, (_, pct_optimal) in binomial_results.items():\n",
        "    name = 'greedy' if eps == 0 else f'epsilon_{int(eps*100):02d}'\n",
        "    ax2.plot(pct_optimal, color=COLORS[name], label=f'ε = {eps}', linewidth=1.5)\n",
        "\n",
        "ax2.set_xlabel('Steps')\n",
        "ax2.set_ylabel('% Optimal Action')\n",
        "ax2.set_title('Binomial Bandit: Optimal Action Selection')\n",
        "ax2.legend(loc='lower right')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.suptitle('Binomial Reward Variant (Binary Outcomes)', fontsize=12, y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcfS46zFfK0n"
      },
      "source": [
        "<div style=\"background: #f8f9fa; padding: 15px 20px; margin-top: 30px; border-left: 3px solid #17a2b8;\">\n",
        "    <h3 style=\"color: #17a2b8; font-size: 14px; margin: 0 0 8px 0; text-transform: uppercase; letter-spacing: 0.5px;\">Summary</h3>\n",
        "    <div style=\"color: #555; line-height: 1.6; font-size: 13px;\">\n",
        "        <p><strong>Key Findings:</strong></p>\n",
        "        <ul style=\"margin: 10px 0; padding-left: 20px;\">\n",
        "            <li>ε-greedy with ε = 0.1 achieves good balance between exploration and exploitation</li>\n",
        "            <li>Pure greedy (ε = 0) often gets stuck on suboptimal actions</li>\n",
        "            <li>Optimistic initialization drives initial exploration even without ε-greedy</li>\n",
        "            <li>Binary rewards (binomial variant) show similar patterns with faster convergence</li>\n",
        "        </ul>\n",
        "    </div>\n",
        "</div>\n",
        "\n",
        "<div style=\"background: #fff3e0; padding: 15px 20px; margin-top: 20px; border-left: 3px solid #ff9800;\">\n",
        "    <h3 style=\"color: #ff9800; font-size: 14px; margin: 0 0 8px 0; text-transform: uppercase; letter-spacing: 0.5px;\">Questions for Reflection</h3>\n",
        "    <ol style=\"color: #555; line-height: 1.8; margin: 8px 0 0 0; padding-left: 20px; font-size: 13px;\">\n",
        "        <li>How would performance change with different numbers of arms (k = 2, 100, 1000)?</li>\n",
        "        <li>What happens to optimistic initialization as the number of steps increases?</li>\n",
        "        <li>How could you adapt ε over time for better performance?</li>\n",
        "        <li>When would you prefer optimistic initialization over ε-greedy exploration?</li>\n",
        "    </ol>\n",
        "</div>\n",
        "\n",
        "<div style=\"background: linear-gradient(90deg, #17a2b8 0%, #0e5a63 60%, #0a3d44 100%); color: white; padding: 15px 20px; margin-top: 30px; text-align: center;\">\n",
        "    <p style=\"margin: 0; font-size: 13px;\">End of Lab 2: Multi-Armed Bandits</p>\n",
        "</div>"
      ]
    }
  ]
}